# ==========================================================
# Docker Compose — Sara AI Core (Phase 11-E Ready)
# ----------------------------------------------------------
# Orchestrates all 4 key services:
#   - web: Flask API (TTS + inference)
#   - streaming: SSE/Twilio streaming server
#   - worker: Celery async processing
#   - redis: state + broker backend
# ==========================================================

version: "3.9"

services:
  # -----------------------------
  # 1️⃣ Web API (Flask + Gunicorn)
  # -----------------------------
  web:
    build: .
    container_name: sara-ai-core-web
    command: >
      sh -c "gunicorn app:app --workers 2 --threads 4
      --bind 0.0.0.0:${WEB_PORT:-5000} --timeout 120 --log-level info"
    ports:
      - "${WEB_PORT:-5000}:${WEB_PORT:-5000}"
    env_file: .env.app
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${WEB_PORT:-5000}/healthz"]
      interval: 20s
      timeout: 5s
      retries: 3
    volumes:
      - shared-audio:/app/public/audio

  # -----------------------------
  # 2️⃣ Streaming Server
  # -----------------------------
  streaming:
    build: .
    container_name: sara-ai-core-streaming
    command: >
      sh -c "gunicorn streaming_server:app --workers 2 --threads 4
      --bind 0.0.0.0:${STREAMING_PORT:-8765} --timeout 120 --log-level info"
    ports:
      - "${STREAMING_PORT:-8765}:${STREAMING_PORT:-8765}"
    env_file: .env.streaming
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${STREAMING_PORT:-8765}/healthz"]
      interval: 20s
      timeout: 5s
      retries: 3
    volumes:
      - shared-audio:/app/public/audio

  # -----------------------------
  # 3️⃣ Celery Worker
  # -----------------------------
  worker:
    build: .
    container_name: sara-ai-core-worker
    command: >
      sh -c "celery -A celery_app.celery worker --loglevel=info --concurrency=2"
    env_file: .env.worker
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    volumes:
      - shared-audio:/app/public/audio

  # -----------------------------
  # 4️⃣ Redis (Broker + Backend)
  # -----------------------------
  redis:
    image: redis:7.2-alpine
    container_name: sara-ai-core-redis
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

# -----------------------------
# Named Volumes
# -----------------------------
volumes:
  redis-data:
  shared-audio:
    driver: local
