# ================================================================
# render.yaml — Sara AI Core (Phase 6 Clean Redeploy - v2)
# ================================================================

version: "1"

services:
  # --------------------------------------------------------
  # Redis — message broker for Celery
  # --------------------------------------------------------
  - name: sara-ai-redis
    type: redis
    plan: standard
    maxmemoryPolicy: allkeys-lru

  # --------------------------------------------------------
  # Core Web App (Flask + Gunicorn + Uvicorn)
  # --------------------------------------------------------
  - name: sara-ai-core-app-v2
    type: web
    env: python
    plan: standard
    region: oregon
    buildCommand: |
      echo "=== Installing dependencies ==="
      pip install --no-cache-dir -r requirements.txt
    startCommand: |
      echo "=== Starting Sara AI Core App ==="
      gunicorn -k uvicorn.workers.UvicornWorker app:app --bind 0.0.0.0:$PORT
    envVars:
      - key: REDIS_URL
        fromService:
          name: sara-ai-redis
          type: redis
          property: connectionString
      - key: WORKER_QUEUE
        value: "sara-ai-core-queue"
      - key: LOG_LEVEL
        value: "info"
      - key: FLASK_ENV
        value: "production"
      - key: PORT
        value: "10000"

  # --------------------------------------------------------
  # Celery Worker (Background Tasks)
  # --------------------------------------------------------
  - name: sara-ai-core-worker-v2
    type: worker
    env: python
    plan: standard
    region: oregon
    buildCommand: |
      echo "=== Installing dependencies for Worker ==="
      pip install --no-cache-dir -r requirements.txt
    startCommand: |
      echo "=== Starting Sara AI Celery Worker ==="
      celery -A celery_app.celery worker --loglevel=info -Q sara-ai-core-queue
    envVars:
      - key: REDIS_URL
        fromService:
          name: sara-ai-redis
          type: redis
          property: connectionString
      - key: WORKER_QUEUE
        value: "sara-ai-core-queue"
      - key: LOG_LEVEL
        value: "info"

  # --------------------------------------------------------
  # Streaming Server (Twilio Media Streams)
  # --------------------------------------------------------
  - name: sara-ai-core-streaming
    type: web
    env: python
    plan: standard
    region: oregon
    buildCommand: |
      echo "=== Installing dependencies for Streaming Server ==="
      pip install --no-cache-dir -r requirements.txt
    startCommand: |
      echo "=== Starting Sara AI Streaming Server ==="
      python -m streaming_server
    envVars:
      - key: REDIS_URL
        fromService:
          name: sara-ai-redis
          type: redis
          property: connectionString
      - key: WORKER_QUEUE
        value: "sara-ai-core-queue"
      - key: LOG_LEVEL
        value: "info"
      - key: PORT
        value: "10001"
