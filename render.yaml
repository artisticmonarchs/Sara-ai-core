# ================================================================
# render.yaml â€” Sara AI Core + Observability (Phase 5C)
# ================================================================
# Defines all core backend services for Sara AI production deployment on Render
# Includes: Flask App API, Celery Worker, Streaming Server, Redis, and Grafana Agent
# ================================================================

version: "1"

services:
  # --------------------------------------------------------
  # Redis Key-Value Service
  # --------------------------------------------------------
  - name: sara-ai-redis
    type: redis
    plan: standard
    maxmemoryPolicy: allkeys-lru

  # --------------------------------------------------------
  # Main Application API (Flask + Uvicorn via Gunicorn)
  # --------------------------------------------------------
  - name: sara-ai-core-app
    type: web
    env: python
    plan: standard
    region: oregon
    buildCommand: pip install -r requirements.txt
    startCommand: >
      gunicorn -k uvicorn.workers.UvicornWorker sara_ai.app:app --bind 0.0.0.0:$PORT
    envVars:
      - key: REDIS_URL
        fromService:
          name: sara-ai-redis
          type: redis
          property: connectionString
      - key: WORKER_QUEUE
        value: "sara-ai-core-queue"
      - key: PYTHONUNBUFFERED
        value: "1"
      - key: LOG_LEVEL
        value: "info"
      - key: SENTRY_DSN
        sync: false
      - key: FLASK_ENV
        value: "production"
      - key: PORT
        value: "10000"

  # --------------------------------------------------------
  # Celery Worker Service
  # --------------------------------------------------------
  - name: sara-ai-core-worker
    type: worker
    env: python
    plan: standard
    region: oregon
    buildCommand: pip install -r requirements.txt
    startCommand: celery -A sara_ai.celery_app.celery worker --loglevel=INFO -Q sara-ai-core-queue
    envVars:
      - key: REDIS_URL
        fromService:
          name: sara-ai-redis
          type: redis
          property: connectionString
      - key: WORKER_QUEUE
        value: "sara-ai-core-queue"
      - key: PYTHONUNBUFFERED
        value: "1"
      - key: LOG_LEVEL
        value: "info"
      - key: SENTRY_DSN
        sync: false

  # --------------------------------------------------------
  # Streaming Server (WebSocket / Twilio Media Streams)
  # --------------------------------------------------------
  - name: sara-ai-core-streaming
    type: web
    env: python
    plan: standard
    region: oregon
    buildCommand: pip install -r requirements.txt
    startCommand: python -m sara_ai.streaming_server
    envVars:
      - key: REDIS_URL
        fromService:
          name: sara-ai-redis
          type: redis
          property: connectionString
      - key: WORKER_QUEUE
        value: "sara-ai-core-queue"
      - key: PYTHONUNBUFFERED
        value: "1"
      - key: LOG_LEVEL
        value: "info"
      - key: SENTRY_DSN
        sync: false
      - key: PORT
        value: "10001"

  # --------------------------------------------------------
  # Grafana Agent (Prometheus + Loki telemetry exporter)
  # --------------------------------------------------------
  - name: sara-ai-agent
    type: web
    env: docker
    plan: standard
    region: oregon
    dockerfilePath: ./Dockerfile.agent
    disk:
      name: agent-config
      mountPath: /etc/agent
    buildCommand: ""
    startCommand: >
      grafana-agent run /etc/agent/config.yaml
    envVars:
      - key: GRAFANA_CLOUD_API_TOKEN
        sync: false
      - key: GRAFANA_CLOUD_REMOTE_WRITE_URL
        value: "https://prometheus-prod-36-prod-us-west-0.grafana.net/api/prom/push"
      - key: GRAFANA_CLOUD_INSTANCE
        value: "noblecomsolutions"
      - key: PORT
        value: "10002"
    autoDeploy: true

# ================================================================
# Notes:
# - Redis: used as broker and transient cache
# - Grafana Agent pushes Prometheus metrics + logs to Grafana Cloud
# - DO NOT hardcode your API token; set it in Render Environment tab
# - Logs and metrics auto-exported with trace_id propagation
# ================================================================
